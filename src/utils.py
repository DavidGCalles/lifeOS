import os
import requests

def available_models():
    """
    Consulta al Proxy LiteLLM para ver quÃ© modelos estÃ¡n activos y disponibles.
    Equivale a listar los modelos de la 'municiÃ³n' actual.
    """
    # 1. Definimos dÃ³nde estÃ¡ el objetivo (el proxy)
    # Por defecto en Docker es http://litellm:4000
    base_url = os.getenv("LITELLM_URL", "http://litellm:4000")
    target_url = f"{base_url}/v1/models"
    
    print(f"\n>>> ðŸ“¡ RADAR: Escaneando modelos en {base_url}...")

    try:
        # 2. Lanzamos el ping al endpoint estÃ¡ndar de OpenAI
        # LiteLLM requiere una key en la cabecera, aunque sea falsa, para validar el formato HTTP
        headers = {"Authorization": "Bearer sk-radar-ping"}
        
        response = requests.get(target_url, headers=headers, timeout=2)
        response.raise_for_status() # Lanza excepciÃ³n si hay error 400/500
        
        # 3. Procesamos la inteligencia recibida
        data = response.json()
        models = data.get("data", [])
        
        if not models:
            print("âš  ALERTA: El proxy responde, pero no reporta modelos activos.")
            return

        print(f"âœ… ENLACE ESTABLECIDO. Arsenal disponible:")
        for m in models:
            # 'id' es el nombre del modelo que debes usar en tu cÃ³digo (ej: openai/crewai-proxy)
            model_id = m.get("id", "Desconocido")
            print(f"   ðŸ”¹ {model_id}")
            
    except requests.exceptions.ConnectionError:
        print(f"ðŸ”¥ ERROR DE CONEXIÃ“N: El proxy en {base_url} no responde.")
        print("   -> Verifica que el contenedor 'litellm' estÃ© corriendo.")
    except Exception as e:
        print(f"âŒ FALLO EN EL ESCANEO: {e}")
    
    print(">>> RADAR FINALIZADO.\n")

if __name__ == "__main__":
    # Para probarlo individualmente desde consola
    available_models()