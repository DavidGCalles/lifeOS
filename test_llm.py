'''
Docstring for test_llm
'''
try:
    from src.llm_config import llm
    print("‚úÖ Configuraci√≥n cargada (src/llm_config.py).")
except ImportError as e:
    print(f"‚ùå Error de importaci√≥n. Aseg√∫rate de haber creado el archivo src/llm_config.py: {e}")
    exit(1)
except Exception as e:
    print(f"‚ùå Error en la configuraci√≥n del LLM: {e}")
    exit(1)

def test_connection():
    '''
    Prueba b√°sica para verificar que el LLM responde.
    '''
    print("\n>>> üß™ INICIANDO PRUEBA DE CONEXI√ìN Y FALLBACK...")
    print(">>> Intentando invocar al modelo primario (o sus reservas)...")
   
    try:
        # Usamos invoke, que es la llamada est√°ndar que usar√° CrewAI por debajo
        msg = "Hola. Confirma que est√°s operativo respondiendo solo con la palabra: OPERATIVO."
        response = llm.invoke(msg)      
        print("\n‚úÖ RESPUESTA RECIBIDA:")
        print("----------------------------------------")
        print(f"Contenido: {response.content}")
        print(f"Modelo (metadata): {response.response_metadata.get('model_name', 'Desconocido (Oculto por Fallback wrapper)')}")
        print("----------------------------------------")
        print(">>> ‚úÖ PRUEBA EXITOSA: El cerebro est√° conectado.")
        
    except Exception as e:
        print("\n‚ùå ERROR CR√çTICO LLAMANDO AL LLM:")
        print(e)
        print("\nPosibles causas:")
        print("1. API Key inv√°lida en .env")
        print("2. Rate Limit agresivo (Si esto pasa, el fallback deber√≠a haber saltado, as√≠ que revisa si TODOS los modelos fallaron).")

if __name__ == "__main__":
    test_connection()